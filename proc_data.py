# Author: Trevor Sherrard
# Since: April 2, 2022
# Project: Capstone
# Purpose: Generate statistics from how model performs over time

import csv
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from shapely.geometry import LineString, Point

def parse_info_vec(info_vec, include_no_fly):
    """
    parses out individual information from the information string
    seen in the inference data CSVs.

    :param info_vec: the string to parse
    :param include_no_fly: try to parse out distance to no-fly zone
    :return info_dict: a dictionary with
    """
    info_dict = dict()
    split_list = info_vec.split(",")

    if(len(split_list) <= 1):
        return dict()

    for i in range(0, len(split_list)):
        cur_elem = split_list[i]

        # split each elem by :
        cur_elem_split = cur_elem.split(":")

        if(not include_no_fly):
            # road reward case
            if(i == 0):
                info_dict["roadReward"] = float(cur_elem_split[-1])

            # height penalty case
            elif(i == 1):
                penalty = cur_elem_split[1]
                penalty_val = penalty.split(" ")[1]
                info_dict["heightPenalty"] = float(penalty_val)

            elif(i == 2):
                backtrack_penalty = cur_elem_split[1]
                info_dict["backtrackPenalty"] = float(backtrack_penalty)

            elif(i == 3):
                distance_from_home = cur_elem_split[1]
                info_dict["distanceFromHome"] = float(distance_from_home.split(" ")[1])
        else:
            # road reward case
            if (i == 0):
                info_dict["roadReward"] = float(cur_elem_split[-1])

            # height penalty case
            elif (i == 1):
                penalty = cur_elem_split[1]
                penalty_val = penalty.split(" ")[1]
                info_dict["heightPenalty"] = float(penalty_val)

            elif (i == 2):
                backtrack_penalty = cur_elem_split[1]
                info_dict["backtrackPenalty"] = float(backtrack_penalty)

            elif (i == 3):
                no_fly_zone_dist = cur_elem_split[1]
                info_dict["noFlyZoneDist"] = float(no_fly_zone_dist)

            elif (i == 4):
                distance_from_home = cur_elem_split[1]
                info_dict["distanceFromHome"] = float(distance_from_home.split(" ")[1])

    return info_dict

def gen_average_episode(data_dict, include_no_fly=False):
    """
    this function attempts to generate an average episode
    from taking a piecewise average of each data stream in
    data_dict over all the episodes

    :param data_dict: the data dictionary generated by the
        parse_data function.
    :param include_no_fly: try to parse out distance to no-fly zone
    :return avg_dict: a dictionary representation of the
        element-wise average for each of the datastreams
        over each episode
    """
    # start by getting minimum vector size
    vec_sizes = list()
    for key in data_dict:
        vec_sizes.append(len(data_dict[key]["x_pos_vec"]))

    min_vec_size = min(vec_sizes)

    # generate vector of vectors
    time_vecs = list()
    x_pos_vecs = list()
    y_pos_vecs = list()
    z_pos_vecs = list()
    reward_vecs = list()
    action_vecs = list()
    road_reward_vecs = list()
    height_penalty_vecs = list()
    backtrack_penalty_vecs = list()
    distance_from_home_vecs = list()
    no_fly_dist_vecs = list()

    for key in data_dict:
        x_pos_vecs.append(data_dict[key]["x_pos_vec"][0:min_vec_size])
        y_pos_vecs.append(data_dict[key]["y_pos_vec"][0:min_vec_size])
        z_pos_vecs.append(data_dict[key]["z_pos_vec"][0:min_vec_size])
        reward_vecs.append(data_dict[key]["reward_vec"][0:min_vec_size])
        action_vecs.append(data_dict[key]["action_vec"][0:min_vec_size])
        time_vecs.append(data_dict[key]["time_vec"][0:min_vec_size])
        road_reward_vecs.append(data_dict[key]["road_reward_vec"][0:min_vec_size])
        height_penalty_vecs.append(data_dict[key]["height_penalty_vec"][0:min_vec_size])
        backtrack_penalty_vecs.append(data_dict[key]["backtrack_penalty_vec"][0:min_vec_size])
        distance_from_home_vecs.append(data_dict[key]["distance_from_home_vec"][0:min_vec_size])

        if(include_no_fly):
            no_fly_dist_vecs.append(data_dict[key]["no_fly_zone_dist_vec"][0:min_vec_size])

    # take average of locations
    x_avg = [(sum(lst) / len(lst)) for lst in zip(*x_pos_vecs)]
    y_avg = [(sum(lst) / len(lst)) for lst in zip(*y_pos_vecs)]
    z_avg = [(sum(lst) / len(lst)) for lst in zip(*z_pos_vecs)]
    reward_avg = [(sum(lst) / len(lst)) for lst in zip(*reward_vecs)]
    action_avg = [(sum(lst) / len(lst)) for lst in zip(*action_vecs)]
    time_avg = [(sum(lst) / len(lst)) for lst in zip(*time_vecs)]
    road_reward_avg = [(sum(lst) / len(lst)) for lst in zip(*road_reward_vecs)]
    height_penalty_avg = [(sum(lst) / len(lst)) for lst in zip(*height_penalty_vecs)]
    backtrack_penalty_avg = [(sum(lst) / len(lst)) for lst in zip(*backtrack_penalty_vecs)]
    distance_from_home_avg = [(sum(lst) / len(lst)) for lst in zip(*distance_from_home_vecs)]

    if(include_no_fly):
        no_fly_zone_dist_avg = [(sum(lst) / len(lst)) for lst in zip(*no_fly_dist_vecs)]

    # construct dictionary
    avg_dict = dict()
    avg_dict["x_avg"] = x_avg
    avg_dict["y_avg"] = y_avg
    avg_dict["z_avg"] = z_avg
    avg_dict["reward_avg"] = reward_avg
    avg_dict["action_avg"] = action_avg
    avg_dict["time_avg"] = time_avg
    avg_dict["road_reward_avg"] = road_reward_avg
    avg_dict["height_penalty_avg"] = height_penalty_avg
    avg_dict["backtrack_penalty_avg"] = backtrack_penalty_avg
    avg_dict["distance_from_home_avg"] = distance_from_home_avg

    if(include_no_fly):
        avg_dict["no_fly_zone_dist_avg"] = no_fly_zone_dist_avg

    return avg_dict

def parse_data(filename, include_no_fly=False):
    """
    parses out inference data vectors from CSV files

    :param filename: the file path for the CSV file containing the data
    :param include_no_fly: try to parse out distance to no-fly zone
    :return data_dict: a dictionary of data streams for inference data.
        the key is the episode number
    """
    data_dict = dict()

    with open(filename, "r") as file_obj:
        csv_reader = csv.reader(file_obj, delimiter=",", quotechar="\"")

        # loop through each row of the CSV file and extract elements
        first_row = True
        first_time = None
        for row in csv_reader:
            # skip first row
            if(first_row):
                first_row = False
                continue

            # extract episode number
            episode_num_str = str(row[-1])

            # extract drone position
            x_pos = float(row[0])
            y_pos = float(row[1])
            z_pos = float(row[2])

            # extract current reward and action taken
            cur_reward = float(row[6])
            cur_action = int(row[10])

            # extract reward breakdown info
            info_vec = row[9]
            info_dict = parse_info_vec(info_vec, include_no_fly)

            # extract current timestamp and make datatime object for it
            time_stamp = int(row[7])/1000000000 # make sure we get it in seconds
            cur_time = datetime.fromtimestamp(time_stamp)

            # see if this is the first elements of this episode
            if(episode_num_str not in data_dict.keys()):
                # create new vectors for data
                drone_x_pos_list = list()
                drone_y_pos_list = list()
                drone_z_pos_list = list()
                reward_vec = list()
                action_vec = list()
                time_vec = list()
                road_reward_vec = list()
                height_penalty_vec = list()
                backtrack_penalty_vec = list()
                distance_from_home_vec = list()
                no_fly_zone_dist_vec = list()

                # append initial values to vectors
                drone_x_pos_list.append(x_pos)
                drone_y_pos_list.append(y_pos)
                drone_z_pos_list.append(z_pos)
                reward_vec.append(cur_reward)
                action_vec.append(cur_action)
                time_vec.append(0)
                road_reward_vec.append(0)
                height_penalty_vec.append(0)
                backtrack_penalty_vec.append(0)
                distance_from_home_vec.append(0)
                no_fly_zone_dist_vec.append(0)

                # make dictionary for this episode
                episode_data_dict = dict()
                episode_data_dict["x_pos_vec"] = drone_x_pos_list
                episode_data_dict["y_pos_vec"] = drone_y_pos_list
                episode_data_dict["z_pos_vec"] = drone_z_pos_list
                episode_data_dict["reward_vec"] = reward_vec
                episode_data_dict["action_vec"] = action_vec
                episode_data_dict["time_vec"] = time_vec
                episode_data_dict["road_reward_vec"] = road_reward_vec
                episode_data_dict["height_penalty_vec"] = height_penalty_vec
                episode_data_dict["backtrack_penalty_vec"] = backtrack_penalty_vec
                episode_data_dict["distance_from_home_vec"] = distance_from_home_vec

                if(include_no_fly):
                    episode_data_dict["no_fly_zone_dist_vec"] = no_fly_zone_dist_vec

                # push this dict. into main dict using the episode number as key
                data_dict[episode_num_str] = episode_data_dict

                # set first timestamp to current timestamp
                first_time = cur_time

            # otherwise, just append to existing vectors
            else:
                data_dict[episode_num_str]["x_pos_vec"].append(x_pos)
                data_dict[episode_num_str]["y_pos_vec"].append(y_pos)
                data_dict[episode_num_str]["z_pos_vec"].append(z_pos)
                data_dict[episode_num_str]["reward_vec"].append(cur_reward)
                data_dict[episode_num_str]["action_vec"].append(cur_action)
                data_dict[episode_num_str]["road_reward_vec"].append(info_dict["roadReward"])
                data_dict[episode_num_str]["height_penalty_vec"].append(info_dict["heightPenalty"])
                data_dict[episode_num_str]["backtrack_penalty_vec"].append(info_dict["backtrackPenalty"])
                data_dict[episode_num_str]["distance_from_home_vec"].append(info_dict["distanceFromHome"])

                if(include_no_fly):
                    data_dict[episode_num_str]["no_fly_zone_dist_vec"].append(info_dict["noFlyZoneDist"])

                # compute elapsed time
                delta_t = (cur_time - first_time)
                data_dict[episode_num_str]["time_vec"].append(delta_t.total_seconds())

    return data_dict

def plot_just_z_data(avg_dict, title, sub_title):
    """
    plots the z-axis data along with height reward

    :param avg_dict: the average dict returned from
        gen_average_episode
    :param title: main title
    :param sub_title: subtitle for plot
    """

    #plot z-values and height reward
    time = avg_dict["time_avg"]
    z_vec = avg_dict["z_avg"]
    height_penalty_vec = avg_dict["height_penalty_avg"]
    plt.plot(time, z_vec, label="UAV height")
    plt.plot(time, height_penalty_vec, label="Height Reward")
    plt.legend(loc="best")
    plt.suptitle(title)
    plt.title(sub_title, fontsize=10)
    plt.xlabel("time (sec)")
    plt.ylabel("Height/Z-Reward")

    plt.show()

def plot_xy_road_reward(avg_dict, title, sub_title):
    """
    plots the xy path of drone along with
    the current road reward.

    :param avg_dict: the average dict returned from
        gen_average_episode
    :param title: main title for plot
    :param sub_title: subtitle for plot
    """

    # road coordinates
    across_horizontal_mid = LineString([Point(-125, 0), Point(125, 0)])
    across_horizontal_top = LineString([Point(-125, 125), Point(125, 125)])
    across_horizontal_bot = LineString([Point(-125, -125), Point(125, -125)])
    middle_vert = LineString([Point(0, -125), Point(0, 125)])
    left_vert = LineString([Point(-125, -125), Point(-125, 125)])
    right_vert = LineString([Point(125, -125), Point(125, 125)])
    side_street = LineString([Point(75, -125), Point(75, -1)])
    missing_bottom = LineString([Point(125, 125), Point(125, 0)])

    # plot road coordinates
    plt.plot(*across_horizontal_top.xy, "k")
    plt.plot(*across_horizontal_mid.xy, "k")
    plt.plot(*across_horizontal_bot.xy, "k")
    plt.plot(*middle_vert.xy, "k")
    plt.plot(*left_vert.xy, "k")
    plt.plot(*right_vert.xy, "k")
    plt.plot(*side_street.xy, "k")
    plt.plot(*missing_bottom.xy, "k")

    x_vec = avg_dict["x_avg"]
    y_vec = avg_dict["y_avg"]
    road_reward_vec = avg_dict["road_reward_avg"]

    plt.scatter(x_vec, y_vec, c=road_reward_vec, label="UAV 2D path")
    plt.text(-120, -85, "black lines are road locations", color="red")
    plt.text(-120, -120, "ligher color: higher reward,\ndarker color: lower reward", color="red")
    plt.ylabel("y position")
    plt.xlabel("x position")
    plt.suptitle(title)
    plt.title(sub_title, fontsize=10)
    plt.show()

def plot_xy_backtracking_penalty(avg_dict, title, sub_title):
    """
    plots the xy path of drone along with
    the current backtracking penalty.

    :param avg_dict: the average dict returned from
        gen_average_episode
    :param title: main title for plot
    :param sub_title: subtitle for plot
    """

    # road coordinates
    across_horizontal_mid = LineString([Point(-125, 0), Point(125, 0)])
    across_horizontal_top = LineString([Point(-125, 125), Point(125, 125)])
    across_horizontal_bot = LineString([Point(-125, -125), Point(125, -125)])
    middle_vert = LineString([Point(0, -125), Point(0, 125)])
    left_vert = LineString([Point(-125, -125), Point(-125, 125)])
    right_vert = LineString([Point(125, -125), Point(125, 125)])
    side_street = LineString([Point(75, -125), Point(75, -1)])
    missing_bottom = LineString([Point(125, 125), Point(125, 0)])

    # plot road coordinates
    plt.plot(*across_horizontal_top.xy, "k")
    plt.plot(*across_horizontal_mid.xy, "k")
    plt.plot(*across_horizontal_bot.xy, "k")
    plt.plot(*middle_vert.xy, "k")
    plt.plot(*left_vert.xy, "k")
    plt.plot(*right_vert.xy, "k")
    plt.plot(*side_street.xy, "k")
    plt.plot(*missing_bottom.xy, "k")

    x_vec = avg_dict["x_avg"]
    y_vec = avg_dict["y_avg"]
    back_tracking_reward_vec = avg_dict["backtrack_penalty_avg"]

    plt.scatter(x_vec, y_vec, c=back_tracking_reward_vec, label="UAV 2D path")
    plt.text(-120, -85, "black lines are road locations", color="red")
    plt.text(-120, -120, "ligher color: higher reward,\ndarker color: lower reward", color="red")
    plt.ylabel("y position")
    plt.xlabel("x position")
    plt.suptitle(title)
    plt.title(sub_title, fontsize=10)
    plt.show()

def plot_total_reward(avg_dict, title, sub_title):
    """
    plots the total reward over time

    :param avg_dict: the average dict returned from
        gen_average_episode
    :param title: main title
    :param sub_title: subtitle for plot
    """

    time = avg_dict["time_avg"]
    reward_vec = avg_dict["reward_avg"]
    plt.plot(time, reward_vec)
    plt.suptitle(title)
    plt.title(sub_title, fontsize=10)
    plt.xlabel("time (sec)")
    plt.ylabel("Total Reward")

    plt.show()

def plot_action_histogram(avg_dict, title, sub_title):
    """
        plots a histogram of the averaged action vector

        :param avg_dict: the average dict returned from
            gen_average_episode
        :param title: main title
        :param sub_title: subtitle for plot
        """
    action_vec = avg_dict["action_avg"]
    bins = np.arange(8) - 0.5
    names = ["rotate right", "move forward", "move right", "climb", "move back", "move left", "descend"]
    plt.hist(action_vec, bins)
    plt.suptitle(title)
    plt.title(sub_title, fontsize=10)
    plt.xlabel("action taken")
    plt.ylabel("frequency")
    plt.xticks(np.arange(7), names, rotation=45)
    plt.show()


# declare CSV files for different training levels
file_dict = dict()
file_dict["just_z_low"] = "data/inference_res_data/just_z_reward/Neighborhood_DDQNAgent_Just_Z-Reward_Low_Pos1_test.csv"
file_dict["just_z_low_and_high"] = "data/inference_res_data/just_z_reward/Neighborhood_DDQNAgent_Just_Z-Reward_Low&High_test.csv"
file_dict["square_road_20_sec"] = "data/inference_res_data/box_road_reward/Neighborhood_DDQNAgent_Z_&_Road_Follow-Reward_w_BackTrack_Penalty_test_20_s.csv"
file_dict["square_road_35_sec"] = "data/inference_res_data/box_road_reward/Neighborhood_DDQNAgent_Z_&_Road_Follow-Reward_w_BackTrack_Penalty35_test.csv"
file_dict["square_road_40_sec"] = "data/inference_res_data/box_road_reward/Neighborhood_DDQNAgent_Z_&_Road_Follow-Reward_w_BackTrack_Penalty_Pos1_40s_test.csv"
file_dict["non_linear_road_40_sec"] = "data/inference_res_data/non_linear_road_reward/Neighborhood_DDQNAgent_Z_&_Nonlinear_Road_Follow-Reward_w_BackTrack_Penalty_Pos1_40s_test.csv"
file_dict["non_linear_road_60_sec"] = "data/inference_res_data/non_linear_road_reward/Neighborhood_DDQNAgent_Z_&_Nonlinear_Road_Follow-Reward_w_BackTrack_Penalty_ImgSeg_Pos1_60s_test.csv"

# parse data
just_z_low_data_dict = parse_data(file_dict["just_z_low"], include_no_fly=False)
just_z_high_data_dict = parse_data(file_dict["just_z_low_and_high"], include_no_fly=False)
square_road_20_data_dict = parse_data(file_dict["square_road_20_sec"], include_no_fly=True)
square_road_35_data_dict = parse_data(file_dict["square_road_35_sec"], include_no_fly=True)
square_road_40_data_dict = parse_data(file_dict["square_road_40_sec"], include_no_fly=True)
non_linear_road_40_data_dict = parse_data(file_dict["non_linear_road_40_sec"], include_no_fly=True)
non_linear_road_60_data_dict = parse_data(file_dict["non_linear_road_60_sec"], include_no_fly=True)

# generate average episode
just_z_low_data_dict_avg = gen_average_episode(just_z_low_data_dict, include_no_fly=False)
just_z_high_data_dict_avg = gen_average_episode(just_z_high_data_dict, include_no_fly=False)
square_road_20_data_avg = gen_average_episode(square_road_20_data_dict, include_no_fly=True)
square_road_35_data_avg = gen_average_episode(square_road_35_data_dict, include_no_fly=True)
square_road_40_data_avg = gen_average_episode(square_road_40_data_dict, include_no_fly=True)
non_linear_road_40_data_avg = gen_average_episode(non_linear_road_40_data_dict, include_no_fly=True)
non_linear_road_60_data_avg = gen_average_episode(non_linear_road_60_data_dict, include_no_fly=True)

# plot z-data data
title = "Achieved Height and Z-Reward Over Time (Average of 10 Inference Episodes)"
sub_title = "For D-DQN Model Trained With Repulsive Height Penalty (Ground Start Position)"
plot_just_z_data(just_z_low_data_dict_avg, title, sub_title)

title = "Achieved Height and Z-Reward Over Time (Average of 10 Inference Episodes)"
sub_title = "For D-DQN Model Trained With Repulsive Height Penalty (Aeriel Start Position)"
plot_just_z_data(just_z_high_data_dict_avg, title, sub_title)

# plot drone path alongside road reward
plot_xy_road_reward(just_z_low_data_dict_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Ground Start Position)")
plot_xy_road_reward(just_z_high_data_dict_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Aeriel Start Position)")
plot_xy_road_reward(square_road_20_data_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (20 second training episodes)")
plot_xy_road_reward(square_road_35_data_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (35 second training episodes)")
plot_xy_road_reward(square_road_40_data_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (40 second training episodes)")
plot_xy_road_reward(non_linear_road_40_data_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")
plot_xy_road_reward(non_linear_road_60_data_avg, "Drone XY Trajectory and Road Reward (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")

# plot backtracking reward
plot_xy_backtracking_penalty(just_z_low_data_dict_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Ground Start Position)")
plot_xy_backtracking_penalty(just_z_high_data_dict_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Aeriel Start Position)")
plot_xy_backtracking_penalty(square_road_20_data_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (20 second training episodes)")
plot_xy_backtracking_penalty(square_road_35_data_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (35 second training episodes)")
plot_xy_backtracking_penalty(square_road_40_data_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (40 second training episodes)")
plot_xy_backtracking_penalty(non_linear_road_40_data_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")
plot_xy_backtracking_penalty(non_linear_road_60_data_avg, "Drone XY Trajectory and Backtracking penalty (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")

# plot the total reward over time
plot_total_reward(just_z_low_data_dict_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Ground Start Position)")
plot_total_reward(just_z_high_data_dict_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Aeriel Start Position)")
plot_total_reward(square_road_20_data_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (20 second training episodes)")
plot_total_reward(square_road_35_data_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (35 second training episodes)")
plot_total_reward(square_road_40_data_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (40 second training episodes)")
plot_total_reward(non_linear_road_40_data_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")
plot_total_reward(non_linear_road_60_data_avg, "Cumulative Reward Versus Time (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")

# plot the histogram of actions taken
plot_action_histogram(just_z_low_data_dict_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Ground Start Position)")
plot_action_histogram(just_z_high_data_dict_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Only Height Penalty (Aeriel Start Position)")
plot_action_histogram(square_road_20_data_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (20 second training episodes)")
plot_action_histogram(square_road_35_data_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (35 second training episodes)")
plot_action_histogram(square_road_40_data_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Windowed Road Reward (40 second training episodes)")
plot_action_histogram(non_linear_road_40_data_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")
plot_action_histogram(non_linear_road_60_data_avg, "Histogram Of Actions Taken (Average of 10 Inference Episodes)",
                    "For Model Trained With Non-Linear Road Reward (40 second training episodes)")





