
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/NeuralNetwork.ipynb
import torch.nn as nn
import re
import timm
from  torch import randn

def _is_pool_type(l): return re.search(r'Pool[123]d$', l.__class__.__name__)

def has_pool_type(m):
    "Return `True` if `m` is a pooling layer or has one in its children"
    if _is_pool_type(m): return True
    for l in m.children():
        if has_pool_type(l): return True
    return False

def _get_first_layer(m):
    "Access first layer of a model"
    c,p,n = m,None,None  # child, parent, name
    for n in next(m.named_parameters())[0].split('.')[:-1]:
        p,c=c,getattr(c,n)
    return c,p,n

def _update_first_layer(model, n_in, pretrained):
    "Change first layer based on number of input channels"
    if n_in == 3: return
    first_layer, parent, name = _get_first_layer(model)
    assert isinstance(first_layer, nn.Conv2d), f'Change of input channels only supported with Conv2d, found {first_layer.__class__.__name__}'
    assert getattr(first_layer, 'in_channels') == 3, f'Unexpected number of input channels, found {getattr(first_layer, "in_channels")} while expecting 3'
    params = {attr:getattr(first_layer, attr) for attr in 'out_channels kernel_size stride padding dilation groups padding_mode'.split()}
    params['bias'] = getattr(first_layer, 'bias') is not None
    params['in_channels'] = n_in
    new_layer = nn.Conv2d(**params)
    if pretrained:
        _load_pretrained_weights(new_layer, first_layer)
    setattr(parent, name, new_layer)

def _load_pretrained_weights(new_layer, previous_layer):
    "Load pretrained weights based on number of input channels"
    n_in = getattr(new_layer, 'in_channels')
    if n_in==1:
        # we take the sum
        new_layer.weight.data = previous_layer.weight.data.sum(dim=1, keepdim=True)
    elif n_in==2:
        # we take first 2 channels + 50%
        new_layer.weight.data = previous_layer.weight.data[:,:2] * 1.5
    else:
        # keep 3 channels weights and set others to null
        new_layer.weight.data[:,:3] = previous_layer.weight.data
        new_layer.weight.data[:,3:].zero_()

def freezeCNNLayers(model):
    "This leaves the head with gradients but sets the layers before the AdaptiveAvgPool2d to not update"
    # there is probably a more elequent way to do this
    for child in model.children():
        try: #'AdaptiveAvgPool2d' not subscriptable if pooling changes this may not work
            _ = child[0]
            for param in child.parameters(): param.requires_grad = False
        except: pass #

def create_timm_body(arch:str, pretrained=True, cut=None, n_in=3):
    "Creates a body from any model in the `timm` library."
    model = timm.create_model(arch, pretrained=pretrained, num_classes=0, global_pool='')
    _update_first_layer(model, n_in, pretrained)
    if cut is None:
        ll = list(enumerate(model.children()))
        cut = next(i for i,o in reversed(ll) if has_pool_type(o))
    if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])
    elif callable(cut): return cut(model)
    else: raise NamedError("cut must be either integer or function")

def ModelMaker(arch='tf_mobilenetv3_small_075', input_channels = 4, num_outputs = 7, dropout=0.2):
    "Creates a custom pretrained CNN"
    body = create_timm_body(arch=arch, pretrained=True, cut=None, n_in=input_channels)
    x = randn(1, input_channels, 224, 224);  #expected image size (mobileNet requires 224x224)
    num_in_features=body(x).shape[1]

    model=nn.Sequential(body,
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        #nn.BatchNorm1d(num_in_features), # nn.LayerNorm may be better see: lesson 31 in Actor/Critic Phil Tabor course
        nn.Linear(in_features=num_in_features,out_features=512, bias=False),
        nn.ReLU(),
        #nn.BatchNorm1d(512),
        nn.Dropout(dropout),
        nn.Linear(in_features=512, out_features=num_outputs, bias=False)
    )
    #freezeCNNLayers(model)

    return model